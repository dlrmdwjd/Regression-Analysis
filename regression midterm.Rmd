---
title: "Regresssion Midterm"
author: "dlrmdwjd"
date: '2022-07-07'
output: word_document
---

###### I changed data file names for convenience.

```{r}

# 1. Crime rate
#### load data
data1 <- read.table("data1.txt")
names(data1) <-c("Y","X")
ord <- order(data1$X)
data1<-data1[ord,]
attach(data1)

## 1(a)
x.bar = mean(data1$X)
y.bar = mean(data1$Y)

b1 <-sum((data1$X-x.bar)*(data1$Y-y.bar))/sum((data1$X-x.bar)^2) 
b0 <- y.bar-b1*x.bar


lm.data1 <- lm(Y~X, data= data1)
plot(X,Y)
lines(X,lm.data1$fitted, col=1, lwd= 2)



#### regression function is y = -170.6xi + 20517.6 + error term. Least Square is pretty good method by gauss markov thm. Thus it gives us good fit.

## 1(b)
### (b) - 1 difference in the mean crime rate for two countries whose high-school grad rates differ by one percentage point 

diff = c(mean((data1[data1$X==64,]$Y)) ,mean((data1[data1$X==65,]$Y))
,mean((data1[data1$X==66,]$Y))
,mean((data1[data1$X==67,]$Y)),
mean((data1[data1$X==68,]$Y)),
 mean((data1[data1$X==73,]$Y)),
 mean((data1[data1$X==74,]$Y)),
 mean((data1[data1$X==75,]$Y)),
 mean((data1[data1$X==76,]$Y)),
 mean((data1[data1$X==77,]$Y)),
 mean((data1[data1$X==78,]$Y)),
 mean((data1[data1$X==79,]$Y)),
 mean((data1[data1$X==80,]$Y)),
 mean((data1[data1$X==81,]$Y)),
 mean((data1[data1$X==82,]$Y)),
 mean((data1[data1$X==83,]$Y)),
 mean((data1[data1$X==84,]$Y)),
 mean((data1[data1$X==85,]$Y)),
 mean((data1[data1$X==87,]$Y)),
 mean((data1[data1$X==88,]$Y)),
 mean((data1[data1$X==89,]$Y)))

mean_diff = 0
for (i in 6:17) {
  mean_diff = mean_diff+ abs(diff[i]-diff[i+1])
}
mean_diff = mean_diff +abs(diff[1]-diff[2]) + abs(diff[2]-diff[3])+abs(diff[3]-diff[4])+
  abs(diff[4]-diff[5])+abs(diff[19]-diff[20])+abs(diff[20]-diff[21])

mean_diff/17

### (b) - 2 

b2 <- data1[data1$X==80,]
summary(b2)

#### mean = 8187

### (b) - 3 10th residual
b3 <- read.table("data1.txt")
names(b3) <-c("Y","X")
resid <- lm.data1$resid
resid
resid[57]

### (b) - 4 
#### refer anova table. MSE = 5552112
sigma = mean(sum((lm.data1$resid)^2)/82)
sigma
```

```{r}
# 2. refer to 1

## 2(a)

#### H0 : B1 = 0, H1 : B1 =/=0
summary(lm.data1)
qt(0.995,82)

#### t value is -4.103, and it is out of (-2.637123,2.637123), we can't conclude H0. p-value: 9.57e-05, thus there is linear association

## 2(b) estimating

x.bar = mean(data1$X)
SSX <- sum((data1$X-x.bar)^2)
MSE = sum((lm.data1$resid)^2)/82
ss = sqrt(MSE/SSX)
est <- c(b1-qt(0.995,82)*ss ,b1+qt(0.995,82)*ss)
est

#### estimates are included in the interval (-280.21182,-60.93856) , it is reasonable estimate

## 2(c) anova

anova(lm.data1)

## 2(d) 

qf(0.99,1,82)

#### F statistics are smaller than qf(0.99) by anova table, can't conclude H0 in (a), p-value for F test and p-value for t test results are same.


## 2(e)

plot(X, Y)
lines(X,lm.data1$fitted, col=1, lwd= 2)

#### by our regression line, if X (high-school diploma rate) increase, Y (crime rate) tend to decrease, which means crime rate reduced when high school graduates introduced into analysis. B1, coefficient is -170.6 (for 10,000). If the percentage of graduates increases by 10 percent, the crime rate decreases by 17 percent. It is relatively large reduction.

## 2(f) 

a = summary(lm.data1)
a$r.squared

#### we can obtain easily by summary

```


```{r}
# 3. refer to 1,2

## 3(a)

#### In GLT, H0 : B0, H1 : B0+B1Xi. H0 is Reduced model and H1 is Full model, H1 includes H0.

## 3(b)

#### SSE(full)
aa = anova(lm.data1)
SSE_f = aa$`Sum Sq`[2]

#### SSE(reduced)
SSE_R = aa$`Sum Sq`[1]+aa$`Sum Sq`[2]

#### (3) df_f
df_f = aa$Df[2]

#### (4) df_r
df_r = aa$Df[1]

#### (5) test statistics for F

MSR = aa$`Mean Sq`[1]
f_star = MSR/(SSE_f/df_f)
f_star

#### (6) decision rule

##### H0 : B0 , H1 : B0+B1Xi


## 3(c)

#### F test for 2(a) and GLT statistics are numerically same. two both F statistics are MSR/MSE. (check f_star value and F value in anova table)


```


```{r}
# 4. solution concentration
data2 <- read.table("data2.txt")
names(data2) <- c("Y","X")
ord <- order(data2$X)
data2<-data2[ord,]
attach(data2)
data2


## 4(a) fit a regression

plot(X,Y)
lm.data2 <- lm(Y~X, data=data2)
lines(X,lm.data2$fitted, col=1, lwd= 2)

## 4(b)

#### decision rule > H0 : B0+B1Xij (reduced model),  H1 : Î¼i (full model)

#### check how much distinct c in data

c_data2 <- length(unique(data2$X))
c_data2
full <- lm(Y~factor(X), data=data2)
smaller <- lm(Y~X , data=data2)

anova(full)
anova(smaller)

#### calculate F statistics
#### pure error is sum sq of full model
test = ((2.9247-0.1574)/3)/(0.0157)
test
qf(0.975,3,10)

### can't conclude H0


## 4(c)
#### regression line is not appropriate

## 4(d)

plot(X,Y)
#### use logarithmic transformation

#### 4(e)

library(car)
boxCox(lm.data2)
pdata2 <- powerTransform(lm.data2)
summary(pdata2)

#### SSE calculate
y.1 <- (data2$Y)^(-0.2)
y.2 <-  (data2$Y)^(-0.1)
y.3 <- data2$Y^(0.1)
y.4 <-  (data2$Y)^(0.2)

trans1 <- data.frame(y.1)
cbind(trans1, data2$X)
fit.y1 <- lm(trans1$y.1~data2$X, data=trans1)
sse1 <- sum((fit.y1$residuals)^2)

trans2 <- data.frame(y.2)
cbind(trans2, data2$X)
fit.y2 <- lm(trans2$y.2~data2$X, data=trans2)
sse2 <- sum((fit.y2$residuals)^2)

trans3 <- data.frame(y.3)
cbind(trans3, data2$X)
fit.y3 <- lm(trans3$y~data2$X, data=trans3)
sse3 <- sum((fit.y3$residuals)^2)

trans4 <- data.frame(y.4)
cbind(trans4, data2$X)
fit.y4 <- lm(trans4$y~data2$X, data=trans4)
sse4 <- sum((fit.y4$residuals)^2)

res = c(sse1,sse2,sse3,sse4)

res

## 4(f)
data2[,"logY"] <- log10(data2$Y)
data2
attach(data2)
lm.data2_2 <- lm(logY~X, data= data2)
summary(lm.data2_2)

#### y = -0.1954x + 0.6549

## 4(g)
plot(X,logY)
lines(X,lm.data2_2$fitted, col=1, lwd= 2)
##### good fit (can see linearity)


## 4(h)
data2.lm <- lm(Y~X , data=data2)
par(mfrow=c(2,2))
qqnorm(data2.lm$resid)
qqline(data2.lm$resid)
with(plot(X, data2.lm$resid), data = data2)
plot(data2.lm$fitted, data2.lm$resid)
plot(data2.lm$resid)

#### we can check linearity, constant variance, normality, independency

## 4(i)
plot(X,Y)
lines(X,data2.lm$fitted, col =2 , lwd=2)

```



```{r}
# 5. production time
## 5(a)
data3 <- read.table("data3.txt")
names(data3) <- c("Y","X")
plot(X,Y)

#### it seems like linear but if we use transformation, we can make more appropriate. use X transformation

## 5(b)

data3[,"sqrt(X)"] <- sqrt(data3$X)
data3
attach(data3)
lm.data3 <- lm(Y~sqrt(X), data= data3)
summary(lm.data3)

#### y = 3.6235x+1.2547

## 5(c)
plot(sqrt(X),Y)
lines(sqrt(X),lm.data3$fitted, col=1, lwd= 2)
#### transformed data and its regression line shows linearity, thus it is good fit.


## 5(d)

lm.data3$resid
par(mfrow=c(1,2))
plot(lm.data3$fitted, lm.data3$resid)
qqnorm(lm.data3$resid)

#### we can observe symmetrical with heavy tails in qqplot, which means data is concentrated at its centre. In residual plot, it is hard to say satisfying linearity


## 5(e)
data3.lm <- lm(Y~X, data=data3)
plot(X,Y)
lines(X, data3.lm$fitted, col=2 , lwd=2)
summary(data3.lm)

#### y = 0.53327X+6.86349
```

